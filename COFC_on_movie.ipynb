{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cluster_utils import ClustersShots, ClustersTracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_facetracks_and_links(shot_data, th_feats=0.1, th_overlap=0.85):\n",
    "    fno = 0\n",
    "    nFaces = len(shot_data)\n",
    "    ls_inds = [0]\n",
    "\n",
    "    ls_cannotlink = []\n",
    "    cl_tracks = ClustersTracks(simThresh=th_overlap, featThresh=th_feats)\n",
    "    \n",
    "    for i in range(1, nFaces):\n",
    "        \n",
    "        fno = shot_data[i].fno\n",
    "        if(fno != shot_data[i-1].fno):\n",
    "            #do something\n",
    "            ls_data = shot_data[ls_inds]\n",
    "            ls_cl = cl_tracks.cluster_online(ls_data)\n",
    "            ls_cannotlink.append(ls_cl)    \n",
    "            ls_inds = [i]\n",
    "        else:\n",
    "            ls_inds.append(i)\n",
    "\n",
    "    ls_tracks = cl_tracks.clusters\n",
    "    qMatrix = np.ones((ls_tracks, ls_tracks))\n",
    "\n",
    "    for l in ls_cannotlink:\n",
    "        for i in range(len(l)):\n",
    "            for j in range(i+1, len(l)):\n",
    "                qMatrix[i,j] = 0\n",
    "                qMatrix[j,i] = 0\n",
    "            \n",
    "    return ls_tracks, qMatrix\n",
    "    \n",
    "def process_shot(clusters_shot, ls_frames, detector, aligner, fnet):\n",
    "    shot_data = extract_bboxes_and_features(ls_frames, detector, aligner, fnet) # list of face_element\n",
    "    face_tracks, qMatrix = get_facetracks_and_links(shot_data, th)\n",
    "    clusters_shot.cluster_online(ls_tracks, qMatrix) #the functionwa in paperwa\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((576, 720, 3), (576, 720, 3))\n"
     ]
    }
   ],
   "source": [
    "path = \"../vids/b_1.mkv\"\n",
    "cap = cv2.VideoCapture(path)\n",
    "aligner, fnet = initialize_deep_models()\n",
    "detector = get_dlib_detector()\n",
    "\n",
    "ret = False\n",
    "\n",
    "ls_frames = []\n",
    "\n",
    "for i in range(2):\n",
    "    ret, frame = cap.read()\n",
    "    assert(ret==True)\n",
    "    if(i==0):\n",
    "        ppframe = frame\n",
    "    if(i==1):\n",
    "        pframe = frame\n",
    "    ls_frames.append(frame)\n",
    "\n",
    "clusters_shot = ClustersShots(3.0)\n",
    "    \n",
    "while(ret == True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    assert(ret==True)\n",
    "    ls_frames.append(frame)\n",
    "    # If shot boundary is detected or clip is more than 100 frames (assuming framerate ~20-30fps), process it\n",
    "    if (shot_boundary(ppframe, pframe, frame) or len(ls_frames) > 100):\n",
    "        process_shot(clusters_shot, ls_frames, detector, aligner, fnet)\n",
    "    #update prev-prev frame and prev-frame\n",
    "    ppframe = pframe\n",
    "    pframe = frame\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
